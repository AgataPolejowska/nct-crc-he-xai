{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/utkuozbulak/pytorch-cnn-visualizations/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_IMG_PATH = \"/Users/agatapolejowska/Desktop/nct-crc-he-xai/CRC-VAL-HE-7K/NORM/NORM-TCGA-ADFIIYIC.tif.jpg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 21 11:09:09 2017\n",
    "\n",
    "@author: Utku Ozbulak - github.com/utkuozbulak\n",
    "\"\"\"\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def convert_to_grayscale(im_as_arr):\n",
    "    \"\"\"\n",
    "        Converts 3d image to grayscale\n",
    "\n",
    "    Args:\n",
    "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
    "\n",
    "    returns:\n",
    "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
    "    \"\"\"\n",
    "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
    "    im_max = np.percentile(grayscale_im, 99)\n",
    "    im_min = np.min(grayscale_im)\n",
    "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
    "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
    "    return grayscale_im\n",
    "\n",
    "\n",
    "def save_gradient_images(gradient, file_name):\n",
    "    \"\"\"\n",
    "        Exports the original gradient image\n",
    "\n",
    "    Args:\n",
    "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
    "        file_name (str): File name to be exported\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "    # Normalize\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    # Save image\n",
    "    path_to_file = os.path.join('../results', file_name + '.png')\n",
    "    save_image(gradient, path_to_file)\n",
    "\n",
    "\n",
    "def save_class_activation_images(org_img, activation_map, file_name):\n",
    "    \"\"\"\n",
    "        Saves cam activation map and activation map on the original image\n",
    "\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        file_name (str): File name of the exported image\n",
    "    \"\"\"\n",
    "    if not os.path.exists('../results'):\n",
    "        os.makedirs('../results')\n",
    "    # Grayscale activation map\n",
    "    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n",
    "    # Save colored heatmap\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_Heatmap.png')\n",
    "    save_image(heatmap, path_to_file)\n",
    "    # Save heatmap on iamge\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_On_Image.png')\n",
    "    save_image(heatmap_on_image, path_to_file)\n",
    "    # SAve grayscale heatmap\n",
    "    path_to_file = os.path.join('../results', file_name+'_Cam_Grayscale.png')\n",
    "    save_image(activation_map, path_to_file)\n",
    "\n",
    "\n",
    "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
    "    \"\"\"\n",
    "        Apply heatmap on image\n",
    "    Args:\n",
    "        org_img (PIL img): Original image\n",
    "        activation_map (numpy arr): Activation map (grayscale) 0-255\n",
    "        colormap_name (str): Name of the colormap\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
    "    no_trans_heatmap = color_map(activation)\n",
    "    # Change alpha channel in colormap to make sure original image is displayed\n",
    "    heatmap = copy.copy(no_trans_heatmap)\n",
    "    heatmap[:, :, 3] = 0.4\n",
    "    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n",
    "    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n",
    "\n",
    "    # Apply heatmap on image\n",
    "    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n",
    "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
    "    return no_trans_heatmap, heatmap_on_image\n",
    "\n",
    "\n",
    "def apply_heatmap(R, sx, sy):\n",
    "    \"\"\"\n",
    "        Heatmap code stolen from https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
    "\n",
    "        This is (so far) only used for LRP\n",
    "    \"\"\"\n",
    "    b = 10*((np.abs(R)**3.0).mean()**(1.0/3))\n",
    "    my_cmap = plt.cm.seismic(np.arange(plt.cm.seismic.N))\n",
    "    my_cmap[:, 0:3] *= 0.85\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "    plt.figure(figsize=(sx, sy))\n",
    "    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    plt.axis('off')\n",
    "    heatmap = plt.imshow(R, cmap=my_cmap, vmin=-b, vmax=b, interpolation='nearest')\n",
    "    return heatmap\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def format_np_output(np_arr):\n",
    "    \"\"\"\n",
    "        This is a (kind of) bandaid fix to streamline saving procedure.\n",
    "        It converts all the outputs to the same format which is 3xWxH\n",
    "        with using sucecssive if clauses.\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n",
    "    \"\"\"\n",
    "    # Phase/Case 1: The np arr only has 2 dimensions\n",
    "    # Result: Add a dimension at the beginning\n",
    "    if len(np_arr.shape) == 2:\n",
    "        np_arr = np.expand_dims(np_arr, axis=0)\n",
    "    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n",
    "    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n",
    "    if np_arr.shape[0] == 1:\n",
    "        np_arr = np.repeat(np_arr, 3, axis=0)\n",
    "    # Phase/Case 3: Np arr is of shape 3xWxH\n",
    "    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n",
    "    if np_arr.shape[0] == 3:\n",
    "        np_arr = np_arr.transpose(1, 2, 0)\n",
    "    # Phase/Case 4: NP arr is normalized between 0-1\n",
    "    # Result: Multiply with 255 and change type to make it saveable by PIL\n",
    "    if np.max(np_arr) <= 1:\n",
    "        np_arr = (np_arr*255).astype(np.uint8)\n",
    "    return np_arr\n",
    "\n",
    "\n",
    "def save_image(im, path):\n",
    "    \"\"\"\n",
    "        Saves a numpy matrix or PIL image as an image\n",
    "    Args:\n",
    "        im_as_arr (Numpy array): Matrix of shape DxWxH\n",
    "        path (str): Path to the image\n",
    "    \"\"\"\n",
    "    if isinstance(im, (np.ndarray, np.generic)):\n",
    "        im = format_np_output(im)\n",
    "        im = Image.fromarray(im)\n",
    "    im.save(path)\n",
    "\n",
    "\n",
    "def preprocess_image(pil_im, resize_im=True):\n",
    "    \"\"\"\n",
    "        Processes image for CNNs\n",
    "\n",
    "    Args:\n",
    "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
    "        resize_im (bool): Resize to 224 or not\n",
    "    returns:\n",
    "        im_as_var (torch variable): Variable that contains processed float tensor\n",
    "    \"\"\"\n",
    "    # Mean and std list for channels (Imagenet)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Ensure or transform incoming image to PIL image\n",
    "    if type(pil_im) != Image.Image:\n",
    "        try:\n",
    "            pil_im = Image.fromarray(pil_im)\n",
    "        except Exception as e:\n",
    "            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n",
    "\n",
    "    # Resize image\n",
    "    if resize_im:\n",
    "        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
    "\n",
    "    im_as_arr = np.float32(pil_im)\n",
    "    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n",
    "    # Normalize the channels\n",
    "    for channel, _ in enumerate(im_as_arr):\n",
    "        im_as_arr[channel] /= 255\n",
    "        im_as_arr[channel] -= mean[channel]\n",
    "        im_as_arr[channel] /= std[channel]\n",
    "    # Convert to float tensor\n",
    "    im_as_ten = torch.from_numpy(im_as_arr).float()\n",
    "    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n",
    "    im_as_ten.unsqueeze_(0)\n",
    "    # Convert to Pytorch variable\n",
    "    im_as_var = Variable(im_as_ten, requires_grad=True)\n",
    "    return im_as_var\n",
    "\n",
    "\n",
    "def recreate_image(im_as_var):\n",
    "    \"\"\"\n",
    "        Recreates images from a torch variable, sort of reverse preprocessing\n",
    "    Args:\n",
    "        im_as_var (torch variable): Image to recreate\n",
    "    returns:\n",
    "        recreated_im (numpy arr): Recreated image in array\n",
    "    \"\"\"\n",
    "    reverse_mean = [-0.485, -0.456, -0.406]\n",
    "    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n",
    "    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n",
    "    for c in range(3):\n",
    "        recreated_im[c] /= reverse_std[c]\n",
    "        recreated_im[c] -= reverse_mean[c]\n",
    "    recreated_im[recreated_im > 1] = 1\n",
    "    recreated_im[recreated_im < 0] = 0\n",
    "    recreated_im = np.round(recreated_im * 255)\n",
    "\n",
    "    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n",
    "    return recreated_im\n",
    "\n",
    "\n",
    "def get_positive_negative_saliency(gradient):\n",
    "    \"\"\"\n",
    "        Generates positive and negative saliency maps based on the gradient\n",
    "    Args:\n",
    "        gradient (numpy arr): Gradient of the operation to visualize\n",
    "\n",
    "    returns:\n",
    "        pos_saliency ( )\n",
    "    \"\"\"\n",
    "    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n",
    "    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n",
    "    return pos_saliency, neg_saliency\n",
    "\n",
    "\n",
    "def get_example_params(example_index):\n",
    "    \"\"\"\n",
    "        Gets used variables for almost all visualizations, like the image, model etc.\n",
    "\n",
    "    Args:\n",
    "        example_index (int): Image id to use from examples\n",
    "\n",
    "    returns:\n",
    "        original_image (numpy arr): Original image read from the file\n",
    "        prep_img (numpy_arr): Processed image\n",
    "        target_class (int): Target class for the image\n",
    "        file_name_to_export (string): File name to export the visualizations\n",
    "        pretrained_model(Pytorch model): Model to use for the operations\n",
    "    \"\"\"\n",
    "    # Pick one of the examples\n",
    "    example_list = ((EXAMPLE_IMG_PATH, 56),\n",
    "                    (EXAMPLE_IMG_PATH, 243),\n",
    "                    (EXAMPLE_IMG_PATH, 72))\n",
    "    img_path = example_list[example_index][0]\n",
    "    target_class = example_list[example_index][1]\n",
    "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
    "    # Read image\n",
    "    original_image = Image.open(img_path).convert('RGB')\n",
    "    # Process image\n",
    "    prep_img = preprocess_image(original_image)\n",
    "    # Define model from directory pytorch bin\n",
    "    pretrained_model = models.alexnet(pretrained=True)\n",
    "    return (original_image,\n",
    "            prep_img,\n",
    "            target_class,\n",
    "            file_name_to_export,\n",
    "            pretrained_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUIDED BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 26 11:23:47 2017\n",
    "\n",
    "@author: Utku Ozbulak - github.com/utkuozbulak\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn import ReLU\n",
    "\n",
    "\n",
    "\n",
    "class GuidedBackprop():\n",
    "    \"\"\"\n",
    "       Produces gradients generated with guided back propagation from the given image\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.forward_relu_outputs = []\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "        # Register hook to the first layer\n",
    "        first_layer = list(self.model.features._modules.items())[0][1]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "    def update_relus(self):\n",
    "        \"\"\"\n",
    "            Updates relu activation functions so that\n",
    "                1- stores output in forward pass\n",
    "                2- imputes zero for gradient values that are less than zero\n",
    "        \"\"\"\n",
    "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "            \"\"\"\n",
    "            If there is a negative gradient, change it to zero\n",
    "            \"\"\"\n",
    "            # Get last forward output\n",
    "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
    "            return (modified_grad_out,)\n",
    "\n",
    "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "            \"\"\"\n",
    "            Store results of forward pass\n",
    "            \"\"\"\n",
    "            self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "        # Loop through layers, hook up ReLUs\n",
    "        for pos, module in self.model.features._modules.items():\n",
    "            if isinstance(module, ReLU):\n",
    "                module.register_backward_hook(relu_backward_hook_function)\n",
    "                module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        # Forward pass\n",
    "        model_output = self.model(input_image)\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        # Backward pass\n",
    "        model_output.backward(gradient=one_hot_output)\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        # [0] to get rid of the first channel (1,3,224,224)\n",
    "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
    "        return gradients_as_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/n5p_mp756952ftqhg9lc__ph0000gn/T/ipykernel_15125/2253426397.py:184: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n",
      "/opt/miniconda3/envs/vicellst-detr/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /Users/agatapolejowska/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
      "100%|██████████| 233M/233M [03:11<00:00, 1.27MB/s] \n",
      "/opt/miniconda3/envs/vicellst-detr/lib/python3.11/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided backprop completed\n"
     ]
    }
   ],
   "source": [
    "target_example = 0  # Snake\n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
    "    get_example_params(target_example)\n",
    "\n",
    "# Guided backprop\n",
    "GBP = GuidedBackprop(pretrained_model)\n",
    "# Get gradients\n",
    "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "# Save colored gradients\n",
    "save_gradient_images(guided_grads, file_name_to_export + '_Guided_BP_color')\n",
    "# Convert to grayscale\n",
    "grayscale_guided_grads = convert_to_grayscale(guided_grads)\n",
    "# Save grayscale gradients\n",
    "save_gradient_images(grayscale_guided_grads, file_name_to_export + '_Guided_BP_gray')\n",
    "# Positive and negative saliency maps\n",
    "pos_sal, neg_sal = get_positive_negative_saliency(guided_grads)\n",
    "save_gradient_images(pos_sal, file_name_to_export + '_pos_sal')\n",
    "save_gradient_images(neg_sal, file_name_to_export + '_neg_sal')\n",
    "print('Guided backprop completed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN LAYER VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Nov 18 23:12:08 2017\n",
    "\n",
    "@author: Utku Ozbulak - github.com/utkuozbulak\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class CNNLayerVisualization():\n",
    "    \"\"\"\n",
    "        Produces an image that minimizes the loss of a convolution\n",
    "        operation for a specific layer and filter\n",
    "    \"\"\"\n",
    "    def __init__(self, model, selected_layer, selected_filter):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.selected_layer = selected_layer\n",
    "        self.selected_filter = selected_filter\n",
    "        self.conv_output = 0\n",
    "        # Create the folder to export images if not exists\n",
    "        if not os.path.exists('../generated'):\n",
    "            os.makedirs('../generated')\n",
    "\n",
    "    def hook_layer(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            # Gets the conv output of the selected filter (from selected layer)\n",
    "            self.conv_output = grad_out[0, self.selected_filter]\n",
    "        # Hook the selected layer\n",
    "        self.model[self.selected_layer].register_forward_hook(hook_function)\n",
    "\n",
    "    def visualise_layer_with_hooks(self):\n",
    "        # Hook the selected layer\n",
    "        self.hook_layer()\n",
    "        # Generate a random image\n",
    "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
    "        # Process image and return variable\n",
    "        processed_image = preprocess_image(random_image, False)\n",
    "        # Define optimizer for the image\n",
    "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
    "        for i in range(1, 31):\n",
    "            optimizer.zero_grad()\n",
    "            # Assign create image to a variable to move forward in the model\n",
    "            x = processed_image\n",
    "            for index, layer in enumerate(self.model):\n",
    "                # Forward pass layer by layer\n",
    "                # x is not used after this point because it is only needed to trigger\n",
    "                # the forward hook function\n",
    "                x = layer(x)\n",
    "                # Only need to forward until the selected layer is reached\n",
    "                if index == self.selected_layer:\n",
    "                    # (forward hook function triggered)\n",
    "                    break\n",
    "            # Loss function is the mean of the output of the selected layer/filter\n",
    "            # We try to minimize the mean of the output of that specific filter\n",
    "            loss = -torch.mean(self.conv_output)\n",
    "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            # Update image\n",
    "            optimizer.step()\n",
    "            # Recreate image\n",
    "            self.created_image = recreate_image(processed_image)\n",
    "            # Save image\n",
    "            if i % 5 == 0:\n",
    "                im_path = '../generated/layer_vis_l' + str(self.selected_layer) + \\\n",
    "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
    "                save_image(self.created_image, im_path)\n",
    "\n",
    "    def visualise_layer_without_hooks(self):\n",
    "        # Process image and return variable\n",
    "        # Generate a random image\n",
    "        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n",
    "        # Process image and return variable\n",
    "        processed_image = preprocess_image(random_image, False)\n",
    "        # Define optimizer for the image\n",
    "        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n",
    "        for i in range(1, 31):\n",
    "            optimizer.zero_grad()\n",
    "            # Assign create image to a variable to move forward in the model\n",
    "            x = processed_image\n",
    "            for index, layer in enumerate(self.model):\n",
    "                # Forward pass layer by layer\n",
    "                x = layer(x)\n",
    "                if index == self.selected_layer:\n",
    "                    # Only need to forward until the selected layer is reached\n",
    "                    # Now, x is the output of the selected layer\n",
    "                    break\n",
    "            # Here, we get the specific filter from the output of the convolution operation\n",
    "            # x is a tensor of shape 1x512x28x28.(For layer 17)\n",
    "            # So there are 512 unique filter outputs\n",
    "            # Following line selects a filter from 512 filters so self.conv_output will become\n",
    "            # a tensor of shape 28x28\n",
    "            self.conv_output = x[0, self.selected_filter]\n",
    "            # Loss function is the mean of the output of the selected layer/filter\n",
    "            # We try to minimize the mean of the output of that specific filter\n",
    "            loss = -torch.mean(self.conv_output)\n",
    "            print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            # Update image\n",
    "            optimizer.step()\n",
    "            # Recreate image\n",
    "            self.created_image = recreate_image(processed_image)\n",
    "            # Save image\n",
    "            if i % 5 == 0:\n",
    "                im_path = '../generated/layer_vis_l' + str(self.selected_layer) + \\\n",
    "                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n",
    "                save_image(self.created_image, im_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/vicellst-detr/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /Users/agatapolejowska/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [04:08<00:00, 2.23MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: 1.29\n",
      "Iteration: 2 Loss: -1.83\n",
      "Iteration: 3 Loss: -8.31\n",
      "Iteration: 4 Loss: -15.39\n",
      "Iteration: 5 Loss: -22.23\n",
      "Iteration: 6 Loss: -28.63\n",
      "Iteration: 7 Loss: -34.57\n",
      "Iteration: 8 Loss: -40.28\n",
      "Iteration: 9 Loss: -45.82\n",
      "Iteration: 10 Loss: -51.23\n",
      "Iteration: 11 Loss: -56.52\n",
      "Iteration: 12 Loss: -61.71\n",
      "Iteration: 13 Loss: -66.82\n",
      "Iteration: 14 Loss: -71.90\n",
      "Iteration: 15 Loss: -76.91\n",
      "Iteration: 16 Loss: -81.88\n",
      "Iteration: 17 Loss: -86.85\n",
      "Iteration: 18 Loss: -91.81\n",
      "Iteration: 19 Loss: -96.74\n",
      "Iteration: 20 Loss: -101.64\n",
      "Iteration: 21 Loss: -106.52\n",
      "Iteration: 22 Loss: -111.41\n",
      "Iteration: 23 Loss: -116.30\n",
      "Iteration: 24 Loss: -121.21\n",
      "Iteration: 25 Loss: -126.14\n",
      "Iteration: 26 Loss: -131.10\n",
      "Iteration: 27 Loss: -136.08\n",
      "Iteration: 28 Loss: -141.07\n",
      "Iteration: 29 Loss: -146.10\n",
      "Iteration: 30 Loss: -151.15\n"
     ]
    }
   ],
   "source": [
    "cnn_layer = 17\n",
    "filter_pos = 5\n",
    "# Fully connected layer is not needed\n",
    "pretrained_model = models.vgg16(pretrained=True).features\n",
    "layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n",
    "\n",
    "# Layer visualization with pytorch hooks\n",
    "layer_vis.visualise_layer_with_hooks()\n",
    "\n",
    "# Layer visualization without pytorch hooks\n",
    "# layer_vis.visualise_layer_without_hooks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANILLA BACKPROPAGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaBackprop():\n",
    "    \"\"\"\n",
    "        Produces gradients generated with vanilla back propagation from the image\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        # Hook the first layer to get the gradient\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "\n",
    "        # Register hook to the first layer\n",
    "        first_layer = list(self.model.features._modules.items())[0][1]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        # Forward\n",
    "        model_output = self.model(input_image)\n",
    "        # Zero grads\n",
    "        self.model.zero_grad()\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        # Backward pass\n",
    "        model_output.backward(gradient=one_hot_output)\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        # [0] to get rid of the first channel (1,3,224,224)\n",
    "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
    "        return gradients_as_arr\n",
    "\n",
    "\n",
    "\n",
    "# Get params\n",
    "target_example = 1  # Snake\n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n",
    "    get_example_params(target_example)\n",
    "# Vanilla backprop\n",
    "VBP = VanillaBackprop(pretrained_model)\n",
    "# Generate gradients\n",
    "vanilla_grads = VBP.generate_gradients(prep_img, target_class)\n",
    "# Save colored gradients\n",
    "save_gradient_images(vanilla_grads, file_name_to_export + '_Vanilla_BP_color')\n",
    "# Convert to grayscale\n",
    "grayscale_vanilla_grads = convert_to_grayscale(vanilla_grads)\n",
    "# Save grayscale gradients\n",
    "save_gradient_images(grayscale_vanilla_grads, file_name_to_export + '_Vanilla_BP_gray')\n",
    "print('Vanilla backprop completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicellst-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
